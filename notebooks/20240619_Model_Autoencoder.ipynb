{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fcd9f8-167a-4758-b08d-b1cb407dd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load special jupyter notebook helpers\n",
    "%matplotlib inline\n",
    "\n",
    "# import libraries we'll use below\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de25f9d-0f48-4f05-b206-158aa1abe586",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_path = '/home/aca10131kr/datasets/01_eeg_fmri_data.h5'\n",
    "\n",
    "# Load the data\n",
    "with h5py.File(data_path, 'r') as f:\n",
    "    eeg_data = np.array(f['eeg_train'][:])\n",
    "    fmri_data = np.array(f['fmri_train'][:])\n",
    "\n",
    "# Normalize the data\n",
    "eeg_data = eeg_data / np.max(eeg_data)\n",
    "fmri_data = fmri_data / np.max(fmri_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "eeg_train, eeg_test, fmri_train, fmri_test = train_test_split(eeg_data, fmri_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalization\n",
    "eeg_train = (eeg_train - np.min(eeg_train)) / (np.max(eeg_train) - np.min(eeg_train))\n",
    "fmri_train = (fmri_train - np.min(fmri_train)) / (np.max(fmri_train) - np.min(fmri_train))\n",
    "eeg_test = (eeg_test - np.min(eeg_test)) / (np.max(eeg_test) - np.min(eeg_test))\n",
    "fmri_test = (fmri_test - np.min(fmri_test)) / (np.max(fmri_test) - np.min(fmri_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa73f747-5bbf-4be1-a6ee-0cc6b4b04780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGfMRIDataset(Dataset):\n",
    "    def __init__(self, eeg_data, fmri_data):\n",
    "        self.eeg_data = eeg_data\n",
    "        self.fmri_data = fmri_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.eeg_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg = self.eeg_data[idx]\n",
    "        fmri = self.fmri_data[idx]\n",
    "        return eeg, fmri\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = EEGfMRIDataset(torch.tensor(eeg_train, dtype=torch.float32), torch.tensor(fmri_train, dtype=torch.float32))\n",
    "test_dataset = EEGfMRIDataset(torch.tensor(eeg_test, dtype=torch.float32), torch.tensor(fmri_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4771c459-c086-4564-91d8-9fa76974be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, stride=2, padding=1),  # (B, 64, 32, 135, 5)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),  # (B, 128, 16, 68, 3)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv3d(128, 256, kernel_size=3, stride=2, padding=1),  # (B, 256, 8, 34, 2)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv3d(256, 512, kernel_size=3, stride=2, padding=1),  # (B, 512, 4, 17, 1)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv3d(512, 1024, kernel_size=3, stride=2, padding=1), # (B, 1024, 2, 9, 1)\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout3d(0.5)  # Dropout layer\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 512, 4, 17, 1)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose3d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 256, 8, 34, 2)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 128, 16, 68, 3)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 64, 32, 135, 5)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose3d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 1, 64, 270, 10)\n",
    "            nn.Upsample(size=(64, 64, 30), mode='trilinear', align_corners=True),  # Ensure final output size matches\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # Change shape from (B, 64, 269, 10, 1) to (B, 1, 64, 269, 10)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = ConvAutoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388dca51-2f14-48fe-9e73-d4371f5fcb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.0828\n",
      "Epoch [2/500], Loss: 0.0625\n",
      "Epoch [3/500], Loss: 0.0624\n",
      "Epoch [4/500], Loss: 0.0585\n",
      "Epoch [5/500], Loss: 0.0120\n",
      "Epoch [6/500], Loss: 0.0082\n",
      "Epoch [7/500], Loss: 0.0077\n",
      "Epoch [8/500], Loss: 0.0077\n",
      "Epoch [9/500], Loss: 0.0076\n",
      "Epoch [10/500], Loss: 0.0076\n",
      "Epoch [11/500], Loss: 0.0075\n",
      "Epoch [12/500], Loss: 0.0071\n",
      "Epoch [13/500], Loss: 0.0064\n",
      "Epoch [14/500], Loss: 0.0060\n",
      "Epoch [15/500], Loss: 0.0059\n",
      "Epoch [16/500], Loss: 0.0057\n",
      "Epoch [17/500], Loss: 0.0056\n",
      "Epoch [18/500], Loss: 0.0055\n",
      "Epoch [19/500], Loss: 0.0054\n",
      "Epoch [20/500], Loss: 0.0054\n",
      "Epoch [21/500], Loss: 0.0053\n",
      "Epoch [22/500], Loss: 0.0053\n",
      "Epoch [23/500], Loss: 0.0052\n",
      "Epoch [24/500], Loss: 0.0051\n",
      "Epoch [25/500], Loss: 0.0051\n",
      "Epoch [26/500], Loss: 0.0050\n",
      "Epoch [27/500], Loss: 0.0050\n",
      "Epoch [28/500], Loss: 0.0049\n",
      "Epoch [29/500], Loss: 0.0049\n",
      "Epoch [30/500], Loss: 0.0048\n",
      "Epoch [31/500], Loss: 0.0047\n",
      "Epoch [32/500], Loss: 0.0047\n",
      "Epoch [33/500], Loss: 0.0046\n",
      "Epoch [34/500], Loss: 0.0045\n",
      "Epoch [35/500], Loss: 0.0045\n",
      "Epoch [36/500], Loss: 0.0044\n",
      "Epoch [37/500], Loss: 0.0043\n",
      "Epoch [38/500], Loss: 0.0043\n",
      "Epoch [39/500], Loss: 0.0043\n",
      "Epoch [40/500], Loss: 0.0042\n",
      "Epoch [41/500], Loss: 0.0041\n",
      "Epoch [42/500], Loss: 0.0041\n",
      "Epoch [43/500], Loss: 0.0040\n",
      "Epoch [44/500], Loss: 0.0040\n",
      "Epoch [45/500], Loss: 0.0039\n",
      "Epoch [46/500], Loss: 0.0039\n",
      "Epoch [47/500], Loss: 0.0038\n",
      "Epoch [48/500], Loss: 0.0038\n",
      "Epoch [49/500], Loss: 0.0037\n",
      "Epoch [50/500], Loss: 0.0037\n",
      "Epoch [51/500], Loss: 0.0036\n",
      "Epoch [52/500], Loss: 0.0035\n",
      "Epoch [53/500], Loss: 0.0035\n",
      "Epoch [54/500], Loss: 0.0034\n",
      "Epoch [55/500], Loss: 0.0034\n",
      "Epoch [56/500], Loss: 0.0034\n",
      "Epoch [57/500], Loss: 0.0033\n",
      "Epoch [58/500], Loss: 0.0033\n",
      "Epoch [59/500], Loss: 0.0033\n",
      "Epoch [60/500], Loss: 0.0032\n",
      "Epoch [61/500], Loss: 0.0032\n",
      "Epoch [62/500], Loss: 0.0032\n",
      "Epoch [63/500], Loss: 0.0032\n",
      "Epoch [64/500], Loss: 0.0031\n",
      "Epoch [65/500], Loss: 0.0031\n",
      "Epoch [66/500], Loss: 0.0030\n",
      "Epoch [67/500], Loss: 0.0030\n",
      "Epoch [68/500], Loss: 0.0030\n",
      "Epoch [69/500], Loss: 0.0029\n",
      "Epoch [70/500], Loss: 0.0029\n",
      "Epoch [71/500], Loss: 0.0029\n",
      "Epoch [72/500], Loss: 0.0028\n",
      "Epoch [73/500], Loss: 0.0028\n",
      "Epoch [74/500], Loss: 0.0028\n",
      "Epoch [75/500], Loss: 0.0028\n",
      "Epoch [76/500], Loss: 0.0027\n",
      "Epoch [77/500], Loss: 0.0027\n",
      "Epoch [78/500], Loss: 0.0027\n",
      "Epoch [79/500], Loss: 0.0026\n",
      "Epoch [80/500], Loss: 0.0026\n",
      "Epoch [81/500], Loss: 0.0026\n",
      "Epoch [82/500], Loss: 0.0026\n",
      "Epoch [83/500], Loss: 0.0025\n",
      "Epoch [84/500], Loss: 0.0025\n",
      "Epoch [85/500], Loss: 0.0025\n",
      "Epoch [86/500], Loss: 0.0024\n",
      "Epoch [87/500], Loss: 0.0024\n",
      "Epoch [88/500], Loss: 0.0024\n",
      "Epoch [89/500], Loss: 0.0024\n",
      "Epoch [90/500], Loss: 0.0024\n",
      "Epoch [91/500], Loss: 0.0024\n",
      "Epoch [92/500], Loss: 0.0023\n",
      "Epoch [93/500], Loss: 0.0023\n",
      "Epoch [94/500], Loss: 0.0023\n",
      "Epoch [95/500], Loss: 0.0023\n",
      "Epoch [96/500], Loss: 0.0023\n",
      "Epoch [97/500], Loss: 0.0022\n",
      "Epoch [98/500], Loss: 0.0022\n",
      "Epoch [99/500], Loss: 0.0022\n",
      "Epoch [100/500], Loss: 0.0022\n",
      "Epoch [101/500], Loss: 0.0021\n",
      "Epoch [102/500], Loss: 0.0021\n",
      "Epoch [103/500], Loss: 0.0021\n",
      "Epoch [104/500], Loss: 0.0021\n",
      "Epoch [105/500], Loss: 0.0021\n",
      "Epoch [106/500], Loss: 0.0021\n",
      "Epoch [107/500], Loss: 0.0021\n",
      "Epoch [108/500], Loss: 0.0020\n",
      "Epoch [109/500], Loss: 0.0020\n",
      "Epoch [110/500], Loss: 0.0020\n",
      "Epoch [111/500], Loss: 0.0020\n",
      "Epoch [112/500], Loss: 0.0020\n",
      "Epoch [113/500], Loss: 0.0020\n",
      "Epoch [114/500], Loss: 0.0020\n",
      "Epoch [115/500], Loss: 0.0020\n",
      "Epoch [116/500], Loss: 0.0020\n",
      "Epoch [117/500], Loss: 0.0020\n",
      "Epoch [118/500], Loss: 0.0020\n",
      "Epoch [119/500], Loss: 0.0020\n",
      "Epoch [120/500], Loss: 0.0019\n",
      "Epoch [121/500], Loss: 0.0019\n",
      "Epoch [122/500], Loss: 0.0019\n",
      "Epoch [123/500], Loss: 0.0019\n",
      "Epoch [124/500], Loss: 0.0019\n",
      "Epoch [125/500], Loss: 0.0019\n",
      "Epoch [126/500], Loss: 0.0019\n",
      "Epoch [127/500], Loss: 0.0019\n",
      "Epoch [128/500], Loss: 0.0019\n",
      "Epoch [129/500], Loss: 0.0019\n",
      "Epoch [130/500], Loss: 0.0019\n",
      "Epoch [131/500], Loss: 0.0019\n",
      "Epoch [132/500], Loss: 0.0019\n",
      "Epoch [133/500], Loss: 0.0019\n",
      "Epoch [134/500], Loss: 0.0018\n",
      "Epoch [135/500], Loss: 0.0018\n",
      "Epoch [136/500], Loss: 0.0018\n",
      "Epoch [137/500], Loss: 0.0018\n",
      "Epoch [138/500], Loss: 0.0018\n",
      "Epoch [139/500], Loss: 0.0018\n",
      "Epoch [140/500], Loss: 0.0018\n",
      "Epoch [141/500], Loss: 0.0018\n",
      "Epoch [142/500], Loss: 0.0018\n",
      "Epoch [143/500], Loss: 0.0018\n",
      "Epoch [144/500], Loss: 0.0018\n",
      "Epoch [145/500], Loss: 0.0018\n",
      "Epoch [146/500], Loss: 0.0018\n",
      "Epoch [147/500], Loss: 0.0018\n",
      "Epoch [148/500], Loss: 0.0018\n",
      "Epoch [149/500], Loss: 0.0018\n",
      "Epoch [150/500], Loss: 0.0018\n",
      "Epoch [151/500], Loss: 0.0017\n",
      "Epoch [152/500], Loss: 0.0017\n",
      "Epoch [153/500], Loss: 0.0017\n",
      "Epoch [154/500], Loss: 0.0017\n",
      "Epoch [155/500], Loss: 0.0017\n",
      "Epoch [156/500], Loss: 0.0017\n",
      "Epoch [157/500], Loss: 0.0017\n",
      "Epoch [158/500], Loss: 0.0017\n",
      "Epoch [159/500], Loss: 0.0017\n",
      "Epoch [160/500], Loss: 0.0017\n",
      "Epoch [161/500], Loss: 0.0017\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Permute labels to match the shape of outputs\n",
    "        labels = labels.permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()  # Adjust learning rate\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfcbab-cafe-49ab-b093-4839ddcef6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get a batch of test inputs\n",
    "test_batch = next(iter(test_loader))\n",
    "inputs, labels = test_batch\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "# Generate outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "inputs_np = inputs.cpu().numpy()\n",
    "labels_np = labels.cpu().numpy()\n",
    "outputs_np = outputs.cpu().numpy()\n",
    "\n",
    "# Permute labels to match the shape of outputs\n",
    "labels_np = np.transpose(labels_np, (0, 4, 1, 2, 3))\n",
    "\n",
    "# Function to plot the data\n",
    "def plot_comparison(labels, outputs, slice_idx, depth_idx):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Select a specific depth slice\n",
    "    labels_slice = labels[slice_idx, 0, :, :, depth_idx]\n",
    "    outputs_slice = outputs[slice_idx, 0, :, :, depth_idx]\n",
    "    diff_slice = labels_slice - outputs_slice\n",
    "\n",
    "    # Ensure the selected slices are 2D\n",
    "    labels_slice = labels_slice if labels_slice.ndim == 2 else labels_slice[:, :, 0]\n",
    "    outputs_slice = outputs_slice if outputs_slice.ndim == 2 else outputs_slice[:, :, 0]\n",
    "    diff_slice = diff_slice if diff_slice.ndim == 2 else diff_slice[:, :, 0]\n",
    "\n",
    "    axes[0].imshow(labels_slice, cmap='gray')\n",
    "    axes[0].set_title('Ground Truth')\n",
    "\n",
    "    axes[1].imshow(outputs_slice, cmap='gray')\n",
    "    axes[1].set_title('Generated Output')\n",
    "\n",
    "    axes[2].imshow(diff_slice, cmap='gray')\n",
    "    axes[2].set_title('Difference')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot a comparison for a specific slice and depth\n",
    "plot_comparison(labels_np, outputs_np, slice_idx=0, depth_idx=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5127fb-e5f6-4a2a-926b-5969ea514f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the data\n",
    "def plot_comparison(labels, outputs, slice_idx):\n",
    "    for depth_idx in range(17):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Select a specific depth slice\n",
    "        labels_slice = labels[slice_idx, 0, :, :, depth_idx]\n",
    "        outputs_slice = outputs[slice_idx, 0, :, :, depth_idx]\n",
    "        diff_slice = labels_slice - outputs_slice\n",
    "\n",
    "        # Ensure the selected slices are 2D\n",
    "        labels_slice = labels_slice if labels_slice.ndim == 2 else labels_slice[:, :, 0]\n",
    "        outputs_slice = outputs_slice if outputs_slice.ndim == 2 else outputs_slice[:, :, 0]\n",
    "        diff_slice = diff_slice if diff_slice.ndim == 2 else diff_slice[:, :, 0]\n",
    "\n",
    "        axes[0].imshow(labels_slice, cmap='gray')\n",
    "        axes[0].set_title('Ground Truth')\n",
    "\n",
    "        axes[1].imshow(outputs_slice, cmap='gray')\n",
    "        axes[1].set_title('Generated Output')\n",
    "\n",
    "        axes[2].imshow(diff_slice, cmap='gray')\n",
    "        axes[2].set_title('Difference')\n",
    "\n",
    "        plt.suptitle(f'Depth Index: {depth_idx}')\n",
    "        plt.show()\n",
    "\n",
    "# Plot a comparison for a specific slice and all depths in range 16\n",
    "plot_comparison(labels_np, outputs_np, slice_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6a51f-d8c2-45d2-b4be-db2e2fe315ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d26037-f891-409b-b26b-8ab13ecd47c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4b083-0532-4d78-9210-81310deeef2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
